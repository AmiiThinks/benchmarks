# FA Dyna Benchmark
#### by Edan Meyer

This benchmark uses a function approximation verison of dyna with an auxillary observation reconstruction loss term. During both rollouts and training, all model passes are done on the GPU. Because there is only one environment instance, fast CPU -> GPU communication speed should have a large effect on performance (which is pretty typical in many RL experiments).

## System Requirements

To run this benchmark, you will need:

- Docker: [Install Docker](https://www.docker.com/get-started)
- NVIDIA Docker support: [Install NVIDIA Docker](https://github.com/NVIDIA/nvidia-docker) (required for GPU use, so required for this benchmark)

## Run Instructions

1. First build the docker container:
`docker build -t dyna-benchmark .`

2. Then run it:
`docker run dyna-benchmark`

After the run is complete, a benchmark report will be printed.

## Interpretation

The printed report will include the time for several different subsections of the test. If possible, include all numbers (except total env steps) in results reports. Each subsection is briefly described here:

- Total Run Time: Run time of the non-startup computation, the lower the better
- Total Env Steps: The fixed number of environment steps run, this can be ignored when interpreting results
- Env Rollouts: The time spent simulating the environment + generating actions with the policy. The CPU speed will affect the environment simulation time. The GPU speed will affect the policy forward pass time. The CPU / GPU bandwidth will affect the time it takes to generate actions, and will likely be the greatest bottleneck.
- Autoencoder Training: Time spent training to VQ-VAE, primarily affected by GPU speed.
- On-Policy QNet Training: Time spent training the QNet, primarily affected by GPU speed.
- Transition Model Training: Time spent training the transition model, primarily affected by GPU speed.
- Model-Based QNet Training: Trains the QNet with information generated by the transition model, primarily affected by GPU speed.

Overall, the rollouts should take the longest time to run due to the CPU / GPU bandwidth. Increasing bandwidth should be the most effective way of improving performance on this benchmark. This type of bottleneck is typical in reinforcement learning jobs that are not parallelized (which is very common when you are a grad student just wanting to run some quick experiments ðŸ˜Š).
